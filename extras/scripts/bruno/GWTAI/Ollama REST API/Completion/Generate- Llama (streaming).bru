meta {
  name: Generate: Llama (streaming)
  type: http
  seq: 5
}

post {
  url: {{baseUrl}}/api/generate
  body: json
  auth: none
}

body:json {
  {
    "model": "llama3.2", //any models pulled from Ollama can be replaced here
    "prompt": "Why is the sky blue?" //The prompt can be written here, or made to be a variable
  }
}

script:pre-request {
  // Initialize a collection variable to store the cumulative response
  bru.setVar("cumulativeResponse", "");
}

tests {
  // Test to check if the response code is 200
  test("Status code is 200", function () {
      expect(res.getStatus()).to.equal(200);
  });
  
}

docs {
  Generate a chat with streaming. This example uses a Llama model.
}
